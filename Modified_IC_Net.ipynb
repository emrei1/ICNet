{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9ocu0d9T8fF",
        "outputId": "57864ff6-5a6a-4798-d462-11aa72f64761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ui7wo3eSVLa",
        "outputId": "6f10724e-e486-4d15-f674-55d24e3ab047"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uSRbKkGbSEiY"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18, resnet34, resnet50, resnet101, resnet152\n",
        "\n",
        "from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights\n",
        "\n",
        "base_model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i0EWIDFObPhr"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "def save_to_drive(source_path):\n",
        "  #print(source_path)\n",
        "  shutil.copy(source_path,\"/content/drive/MyDrive/multiclass-seg/icnet-scale1-base-20Epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tX1OJ4qNlD4H"
      },
      "outputs": [],
      "source": [
        "\"\"\"Base Model for Semantic Segmentation\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "__all__ = ['SegBaseModel']\n",
        "\n",
        "class SegBaseModel(nn.Module):\n",
        "    \"\"\"Base Model for Semantic Segmentation\n",
        "    Parameters\n",
        "    ----------\n",
        "    backbone : string\n",
        "        Pre-trained dilated backbone network type (default:'resnet50'; 'resnet50',\n",
        "        'resnet101' or 'resnet152').\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nclass, backbone='resnet50', pretrained_base=True, **kwargs):\n",
        "        super(SegBaseModel, self).__init__()\n",
        "        dilated = True # Not used.\n",
        "        self.nclass = nclass\n",
        "        if backbone == 'resnet18':\n",
        "          self.pretrained = base_model\n",
        "        elif backbone == 'resnet34':\n",
        "          self.pretrained = base_model\n",
        "        elif backbone == 'resnet50':\n",
        "          self.pretrained = base_model\n",
        "        elif backbone == 'resnet101':\n",
        "          self.pretrained = base_model\n",
        "        elif backbone == 'resnet152':\n",
        "          self.pretrained = base_model\n",
        "        else:\n",
        "            raise RuntimeError('unknown backbone: {}'.format(backbone))\n",
        "\n",
        "    def base_forward(self, x):\n",
        "        \"\"\"forwarding pre-trained network\"\"\"\n",
        "        x = self.pretrained.conv1(x)\n",
        "        x = self.pretrained.bn1(x)\n",
        "        x = self.pretrained.relu(x)\n",
        "        x = self.pretrained.maxpool(x)\n",
        "        c1 = self.pretrained.layer1(x)\n",
        "        c2 = self.pretrained.layer2(c1)\n",
        "        c3 = self.pretrained.layer3(c2)\n",
        "        c4 = self.pretrained.layer4(c3)\n",
        "        \n",
        "        return c1, c2, c3, c4\n",
        "\n",
        "    def evaluate(self, x):\n",
        "        \"\"\"evaluating network with inputs and targets\"\"\"\n",
        "        return self.forward(x)[0]\n",
        "\n",
        "    def demo(self, x):\n",
        "        pred = self.forward(x)\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59AdArZxTKkB"
      },
      "source": [
        "# ICNET MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YUCFOh_2k3nO"
      },
      "outputs": [],
      "source": [
        "\"\"\"Image Cascade Network\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#from .segbase import SegBaseModel\n",
        "from torchsummary import summary\n",
        "\n",
        "__all__ = ['ICNet', 'get_icnet', 'get_icnet_resnet50_citys',\n",
        "           'get_icnet_resnet101_citys', 'get_icnet_resnet152_citys']\n",
        "\n",
        "class ICNet(SegBaseModel):\n",
        "    \"\"\"Image Cascade Network\"\"\"\n",
        "    \n",
        "    def __init__(self, nclass = 19, backbone='resnet50', pretrained_base=True):\n",
        "        super(ICNet, self).__init__(nclass,backbone, pretrained_base=pretrained_base)\n",
        "        self.conv_sub1 = nn.Sequential(\n",
        "            _ConvBNReLU(3, 32, 3, 2),#_ConvBNReLU(3, 128, 3, 2),  #in,out,kernel_size,stride\n",
        "            _ConvBNReLU(32, 32, 3, 2),#_ConvBNReLU(128, 128, 3, 2),\n",
        "            _ConvBNReLU(32, 64, 3, 2)#_ConvBNReLU(128, 64, 3, 2)\n",
        "        )\n",
        "        \n",
        "        self.ppm = PyramidPoolingModule()\n",
        "\n",
        "        self.head = _ICHead(nclass, backbone)\n",
        "\n",
        "        self.__setattr__('exclusive', ['conv_sub1', 'head'])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # sub 1\n",
        "        x_sub1 = self.conv_sub1(x)\n",
        "\n",
        "        # sub 2\n",
        "        x_sub2 = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=True) # scale_factor = 0.5\n",
        "        _, x_sub2, _, _ = self.base_forward(x_sub2)\n",
        "        \n",
        "        # sub 4\n",
        "        x_sub4 = F.interpolate(x, scale_factor=0.25, mode='bilinear', align_corners=True) # scale_factor = 0.25\n",
        "        _, _, _, x_sub4 = self.base_forward(x_sub4)\n",
        "        # add PyramidPoolingModule\n",
        "        x_sub4 = self.ppm(x_sub4)\n",
        "        \n",
        "        outputs = self.head(x_sub1, x_sub2, x_sub4)\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        return tuple(outputs)\n",
        "\n",
        "class PyramidPoolingModule(nn.Module):\n",
        "\tdef __init__(self, pyramids=[1,2,3,6]):\n",
        "\t\tsuper(PyramidPoolingModule, self).__init__()\n",
        "\t\tself.pyramids = pyramids\n",
        "\n",
        "\tdef forward(self, input):\n",
        "\t\tfeat = input\n",
        "\t\theight, width = input.shape[2:]\n",
        "\t\tfor bin_size in self.pyramids:\n",
        "\t\t\tx = F.adaptive_avg_pool2d(input, output_size=bin_size)\n",
        "\t\t\tx = F.interpolate(x, size=(height, width), mode='bilinear', align_corners=True)\n",
        "\t\t\tfeat  = feat + x\n",
        "\t\treturn feat\n",
        "    \n",
        "class _ICHead(nn.Module):\n",
        "    def __init__(self, nclass, backbone='resnet50', norm_layer=nn.BatchNorm2d, **kwargs):\n",
        "        super(_ICHead, self).__init__()\n",
        "        if backbone == 'resnet18':\n",
        "          #self.cff_12 = CascadeFeatureFusion(128, 64, 128, nclass, norm_layer, **kwargs) # For ResNet-50 and bigger\n",
        "          self.cff_12 = CascadeFeatureFusion(128, 64, 128, nclass, norm_layer, **kwargs) # For ResNet-34 and smaller \n",
        "          #                               cff_24:out/xsub1:in/cff_12:out\n",
        "          #self.cff_24 = CascadeFeatureFusion(2048, 512, 128, nclass, norm_layer, **kwargs) # For ResNet-50 and bigger\n",
        "          self.cff_24 = CascadeFeatureFusion(512, 128, 128, nclass, norm_layer, **kwargs) # For ResNet-34 and smaller\n",
        "          #                               xsub4:in/xsub2:in/cff_24:out\n",
        "          #                               512, 128, 128, ...\n",
        "        elif backbone == 'resnet34':\n",
        "          self.cff_12 = CascadeFeatureFusion(128, 64, 128, nclass, norm_layer, **kwargs)\n",
        "          self.cff_24 = CascadeFeatureFusion(512, 128, 128, nclass, norm_layer, **kwargs)\n",
        "        elif backbone == 'resnet50':\n",
        "          self.cff_12 = CascadeFeatureFusion(128, 64, 128, nclass, norm_layer, **kwargs)\n",
        "          self.cff_24 = CascadeFeatureFusion(2048, 512, 128, nclass, norm_layer, **kwargs)\n",
        "        elif backbone == 'resnet101':\n",
        "          self.cff_12 = CascadeFeatureFusion(128, 64, 128, nclass, norm_layer, **kwargs)\n",
        "          self.cff_24 = CascadeFeatureFusion(2048, 512, 128, nclass, norm_layer, **kwargs)\n",
        "        elif backbone == 'resnet152':\n",
        "          self.cff_12 = CascadeFeatureFusion(128, 64, 128, nclass, norm_layer, **kwargs)\n",
        "          self.cff_24 = CascadeFeatureFusion(2048, 512, 128, nclass, norm_layer, **kwargs)\n",
        "        \n",
        "        self.conv_cls = nn.Conv2d(128, nclass, 1, bias=False)\n",
        "\n",
        "    def forward(self, x_sub1, x_sub2, x_sub4):\n",
        "        outputs = list()\n",
        "        x_cff_24, x_24_cls = self.cff_24(x_sub4, x_sub2)\n",
        "        outputs.append(x_24_cls)\n",
        "        # x_cff_12, x_12_cls = self.cff_12(x_sub2, x_sub1)\n",
        "        x_cff_12, x_12_cls = self.cff_12(x_cff_24, x_sub1)\n",
        "        outputs.append(x_12_cls)\n",
        "\n",
        "        up_x2 = F.interpolate(x_cff_12, scale_factor=2, mode='bilinear', align_corners=True) # scale_factor = 2\n",
        "        up_x2 = self.conv_cls(up_x2)\n",
        "        outputs.append(up_x2)\n",
        "        up_x8 = F.interpolate(up_x2, scale_factor=4, mode='bilinear', align_corners=True) # scale_factor = 4\n",
        "        outputs.append(up_x8)\n",
        "        # 1 -> 1/4 -> 1/8 -> 1/16\n",
        "        outputs.reverse()\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class _ConvBNReLU(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1,\n",
        "                 groups=1, norm_layer=nn.BatchNorm2d, bias=False, **kwargs):\n",
        "        super(_ConvBNReLU, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n",
        "        self.bn = norm_layer(out_channels)\n",
        "        self.relu = nn.ReLU(True)#self.relu = nn.GELU()#self.relu = nn.LeakyReLU(0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CascadeFeatureFusion(nn.Module):\n",
        "    \"\"\"CFF Unit\"\"\"\n",
        "\n",
        "    def __init__(self, low_channels, high_channels, out_channels, nclass, norm_layer=nn.BatchNorm2d, **kwargs):\n",
        "        super(CascadeFeatureFusion, self).__init__()\n",
        "        self.conv_low = nn.Sequential(\n",
        "            nn.Conv2d(low_channels, out_channels, 3, padding=2, dilation=2, bias=False),\n",
        "            norm_layer(out_channels)\n",
        "        )\n",
        "        self.conv_high = nn.Sequential(\n",
        "            nn.Conv2d(high_channels, out_channels, 1, bias=False),\n",
        "            norm_layer(out_channels)\n",
        "        )\n",
        "        self.conv_low_cls = nn.Conv2d(out_channels, nclass, 1, bias=False)\n",
        "\n",
        "    def forward(self, x_low, x_high):\n",
        "        x_low = F.interpolate(x_low, size=x_high.size()[2:], mode='bilinear', align_corners=True)\n",
        "        x_low = self.conv_low(x_low)\n",
        "\n",
        "        x_high = self.conv_high(x_high)\n",
        "        x = x_low + x_high\n",
        "        x = F.relu(x, inplace=True)#x = F.gelu(x)#x = F.leaky_relu(x, 0.05,inplace=True)\n",
        "        x_low_cls = self.conv_low_cls(x_low)\n",
        "\n",
        "        return x, x_low_cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "guT2zM_iZeuz"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "\n",
        "__all__ = ['SetupLogger']\n",
        "\n",
        "# reference from: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/utils/logger.py\n",
        "def SetupLogger(name, save_dir, distributed_rank, filename=\"log.txt\", mode='w'):\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    # don't log results for the non-master process\n",
        "    if distributed_rank > 0:\n",
        "        return logger\n",
        "    ch = logging.StreamHandler(stream=sys.stdout)\n",
        "    ch.setLevel(logging.DEBUG)\n",
        "    formatter = logging.Formatter(\"%(asctime)s %(name)s %(levelname)s: %(message)s\")\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "    \n",
        "    if save_dir:\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        fh = logging.FileHandler(os.path.join(save_dir, filename), mode=mode)  # 'a+' for add, 'w' for overwrite\n",
        "        fh.setLevel(logging.DEBUG)\n",
        "        fh.setFormatter(formatter)\n",
        "        logger.addHandler(fh)\n",
        "\n",
        "    return "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yUrnWiFzlYE4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "\n",
        "#from dataset import CityscapesDataset\n",
        "#from models import ICNet\n",
        "#from utils import ICNetLoss, IterationPolyLR, SegmentationMetric, SetupLogger\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, cfg, load = False, loadPath = None):\n",
        "        #self.logger = logger\n",
        "        self.cfg = cfg\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.dataparallel = torch.cuda.device_count() > 1\n",
        "        self.losses = []\n",
        "        self.IoU_list = []\n",
        "        \n",
        "        # dataset and dataloader\n",
        "        train_dataset = CityscapesDataset(root = cfg[\"train\"][\"cityscapes_root\"], \n",
        "                                          split='train', \n",
        "                                          base_size=cfg[\"model\"][\"base_size\"], \n",
        "                                          crop_size=cfg[\"model\"][\"crop_size\"])\n",
        "        val_dataset = CityscapesDataset(root = cfg[\"train\"][\"cityscapes_root\"], \n",
        "                                        split='val',\n",
        "                                        base_size=cfg[\"model\"][\"base_size\"], \n",
        "                                        crop_size=cfg[\"model\"][\"crop_size\"])\n",
        "        self.train_dataloader = data.DataLoader(dataset=train_dataset,\n",
        "                                                batch_size=cfg[\"train\"][\"train_batch_size\"],\n",
        "                                                shuffle=True,\n",
        "                                                num_workers=4,\n",
        "                                                pin_memory=True,\n",
        "                                                drop_last=False)\n",
        "        self.val_dataloader = data.DataLoader(dataset=val_dataset,\n",
        "                                              batch_size=cfg[\"train\"][\"valid_batch_size\"],\n",
        "                                              shuffle=False,\n",
        "                                              num_workers=4,\n",
        "                                              pin_memory=True,\n",
        "                                              drop_last=False)\n",
        "        \n",
        "        self.iters_per_epoch = len(self.train_dataloader)\n",
        "        self.max_iters = cfg[\"train\"][\"epochs\"] * self.iters_per_epoch\n",
        "\n",
        "        # create network\n",
        "        self.model = ICNet(nclass = train_dataset.NUM_CLASS, backbone='resnet50').to(self.device) # Backbone choice made here.\n",
        "        \n",
        "        # create criterion\n",
        "        self.criterion = ICNetLoss(ignore_index=train_dataset.IGNORE_INDEX).to(self.device)\n",
        "        \n",
        "        # optimizer, for model just includes pretrained, head and auxlayer\n",
        "        params_list = list()\n",
        "        if hasattr(self.model, 'pretrained'):\n",
        "            params_list.append({'params': self.model.pretrained.parameters(), 'lr': cfg[\"optimizer\"][\"init_lr\"]})\n",
        "        if hasattr(self.model, 'exclusive'):\n",
        "            for module in self.model.exclusive:\n",
        "                params_list.append({'params': getattr(self.model, module).parameters(), 'lr': cfg[\"optimizer\"][\"init_lr\"] * 10})\n",
        "        # Base optimizer.\n",
        "        self.optimizer = torch.optim.SGD(params = self.model.parameters(),\n",
        "                                          lr = cfg[\"optimizer\"][\"init_lr\"],\n",
        "                                          momentum=cfg[\"optimizer\"][\"momentum\"],\n",
        "                                          weight_decay=cfg[\"optimizer\"][\"weight_decay\"])\n",
        "        \n",
        "        # AdamW optimizer.\n",
        "        #self.optimizer = torch.optim.AdamW(params = params_list, \n",
        "        #                                   lr = 1e-2,\n",
        "        #                                   betas = (0.9, 0.999),\n",
        "        #                                   eps = 1e-8,\n",
        "        #                                   weight_decay = 1e-2)\n",
        "\n",
        "        # Base optimizer.\n",
        "        # self.optimizer = torch.optim.SGD(params = self.model.parameters(),\n",
        "        #                                  lr = cfg[\"optimizer\"][\"init_lr\"],\n",
        "        #                                  momentum=cfg[\"optimizer\"][\"momentum\"],\n",
        "        #                                  weight_decay=cfg[\"optimizer\"][\"weight_decay\"])\n",
        "        \n",
        "        # LambdaLR scheduler.\n",
        "        #schedule_fn = lambda epoch: 0.95 ** epoch\n",
        "        #self.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=schedule_fn)\n",
        "        \n",
        "        # base lr scheduler\n",
        "        self.lr_scheduler = IterationPolyLR(self.optimizer,\n",
        "                                            max_iters=self.max_iters,\n",
        "                                            power=0.9)\n",
        "        \n",
        "        # dataparallel\n",
        "        if(self.dataparallel):\n",
        "             self.model = nn.DataParallel(self.model)\n",
        "\n",
        "        # evaluation metrics\n",
        "        self.metric = SegmentationMetric(train_dataset.NUM_CLASS)\n",
        "\n",
        "        self.current_mIoU = 0.0\n",
        "        self.best_mIoU = 0.0\n",
        "        \n",
        "        self.epochs = cfg[\"train\"][\"epochs\"]\n",
        "        self.current_epoch = 0\n",
        "        self.current_iteration = 0\n",
        "\n",
        "        if load == True:\n",
        "          checkpoint = torch.load(load_path) ###### TODO ######\n",
        "          self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "          self.optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
        "          self.current_epoch = checkpoint['current_epoch']\n",
        "        \n",
        "    def train(self):\n",
        "        epochs, max_iters = self.epochs, self.max_iters\n",
        "        log_per_iters = self.cfg[\"train\"][\"log_iter\"]\n",
        "        val_per_iters = self.cfg[\"train\"][\"val_epoch\"] * self.iters_per_epoch\n",
        "        \n",
        "        start_time = time.time()\n",
        "        print('Start training, Total Epochs: {:d} = Total Iterations {:d}'.format(epochs, max_iters))\n",
        "        #logger.info('Start training, Total Epochs: {:d} = Total Iterations {:d}'.format(epochs, max_iters))\n",
        "        \n",
        "        self.model.train()\n",
        "        \n",
        "        for _ in range(self.epochs): \n",
        "            self.current_epoch += 1\n",
        "            lsit_pixAcc = []\n",
        "            list_mIoU = []\n",
        "            list_loss = []\n",
        "            self.metric.reset()\n",
        "            for i, (images, targets, _) in enumerate(self.train_dataloader):  \n",
        "                self.current_iteration += 1\n",
        "                \n",
        "                self.lr_scheduler.step() # Use this for the base lr scheduler.\n",
        "\n",
        "                images = images.to(self.device)\n",
        "                targets = targets.to(self.device)\n",
        "                \n",
        "                outputs = self.model(images)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "\t\t\n",
        "                self.metric.update(outputs[0], targets)\n",
        "                pixAcc, mIoU = self.metric.get()\n",
        "                lsit_pixAcc.append(pixAcc)\n",
        "                list_mIoU.append(mIoU)\n",
        "                list_loss.append(loss.item())\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                eta_seconds = ((time.time() - start_time) / self.current_iteration) * (max_iters - self.current_iteration)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                \n",
        "                if self.current_iteration % log_per_iters == 0:\n",
        "                  print(\"Epochs: {:d}/{:d} || Iters: {:d}/{:d} || Lr: {:.6f} || Train Loss: {:.4f} || mIoU: {:.4f} || Cost Time: {} || Estimated Time: {}\".format(\n",
        "                            self.current_epoch, self.epochs, \n",
        "                            self.current_iteration, max_iters, \n",
        "                            self.optimizer.param_groups[0]['lr'], \n",
        "                            loss.item(), \n",
        "                            mIoU,\n",
        "                            str(datetime.timedelta(seconds=int(time.time() - start_time))), \n",
        "                            eta_string))\n",
        "                    #logger.info(\n",
        "                     #   \"Epochs: {:d}/{:d} || Iters: {:d}/{:d} || Lr: {:.6f} || Loss: {:.4f} || mIoU: {:.4f} || Cost Time: {} || Estimated Time: {}\".format(\n",
        "                      #      self.current_epoch, self.epochs, \n",
        "                       #     self.current_iteration, max_iters, \n",
        "                        #    self.optimizer.param_groups[0]['lr'], \n",
        "                         #   loss.item(), \n",
        "                          #  mIoU,\n",
        "                           # str(datetime.timedelta(seconds=int(time.time() - start_time))), \n",
        "                            #eta_string))\n",
        "\t\t\n",
        "            #self.lr_scheduler.step() # Use this for the lr schedulers which changes with epoch.\n",
        "\n",
        "            average_pixAcc = sum(lsit_pixAcc)/len(lsit_pixAcc)\n",
        "            average_mIoU = sum(list_mIoU)/len(list_mIoU)\n",
        "            average_loss = sum(list_loss)/len(list_loss)\n",
        "            self.losses.append(average_loss)\n",
        "            self.IoU_list.append(average_mIoU)\n",
        "            print(\"Epochs: {:d}/{:d}, Average loss: {:.3f}, Average mIoU: {:.3f}, Average pixAcc: {:.3f}\".format(self.current_epoch, self.epochs, average_loss, average_mIoU, average_pixAcc))\n",
        "            #logger.info(\"Epochs: {:d}/{:d}, Average loss: {:.3f}, Average mIoU: {:.3f}, Average pixAcc: {:.3f}\".format(self.current_epoch, self.epochs, average_loss, average_mIoU, average_pixAcc))\n",
        "\t\t\n",
        "            if self.current_iteration % val_per_iters == 0:\n",
        "                self.validation()\n",
        "                self.model.train()\n",
        "\n",
        "        total_training_time = time.time() - start_time\n",
        "        total_training_str = str(datetime.timedelta(seconds=total_training_time))\n",
        "        print(\"Total training time: {} ({:.4f}s / it)\".format(\n",
        "            total_training_str, total_training_time / max_iters))\n",
        "        #logger.info(\n",
        "         #   \"Total training time: {} ({:.4f}s / it)\".format(\n",
        "          #  total_training_str, total_training_time / max_iters))\n",
        "        return self.model\n",
        "\n",
        "    def validation(self):\n",
        "        is_best = False\n",
        "        self.metric.reset()\n",
        "        if self.dataparallel:\n",
        "            model = self.model.module\n",
        "        else:\n",
        "            model = self.model\n",
        "        model.eval()\n",
        "        lsit_pixAcc = []\n",
        "        list_mIoU = []\n",
        "        list_loss = []\n",
        "        for i, (image, targets, filename) in enumerate(self.val_dataloader):\n",
        "            image = image.to(self.device)\n",
        "            targets = targets.to(self.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                outputs = model(image)\n",
        "                loss = self.criterion(outputs, targets)\n",
        "            self.metric.update(outputs[0], targets)\n",
        "            pixAcc, mIoU = self.metric.get()\n",
        "            lsit_pixAcc.append(pixAcc)\n",
        "            list_mIoU.append(mIoU)\n",
        "            list_loss.append(loss.item())\n",
        "\n",
        "        average_pixAcc = sum(lsit_pixAcc)/len(lsit_pixAcc)\n",
        "        average_mIoU = sum(list_mIoU)/len(list_mIoU)\n",
        "        average_loss = sum(list_loss)/len(list_loss)\n",
        "        self.current_mIoU = average_mIoU\n",
        "        print(\"Validation: Average loss: {:.3f}, Average mIoU: {:.3f}, Average pixAcc: {:.3f}\".format(average_loss,  average_mIoU, average_pixAcc))\n",
        "        #self.logger.info(\"Validation: Average loss: {:.3f}, Average mIoU: {:.3f}, Average pixAcc: {:.3f}\".format(average_loss,  average_mIoU, average_pixAcc))\n",
        "\n",
        "        save_checkpoint(self, self.model, self.optimizer, average_mIoU, average_loss, self.cfg, self.current_epoch, is_best, self.current_mIoU, self.dataparallel)\n",
        "        \n",
        "        #if self.current_mIoU > self.best_mIoU:\n",
        "        #    is_best = True\n",
        "        #    self.best_mIoU = self.current_mIoU\n",
        "        #if is_best:\n",
        "           # save_checkpoint(self.model, self.optimizer, self.losses, self.cfg, self.current_epoch, is_best, self.current_mIoU, self.dataparallel)\n",
        "\n",
        "def save_checkpoint(trainer, model, optimizer, valid_mIoU, valid_loss, cfg, epoch = 0, is_best=False, mIoU = 0.0, dataparallel = False):\n",
        "    \"\"\"Save Checkpoint\"\"\"\n",
        "    directory = os.path.expanduser(cfg[\"train\"][\"ckpt_dir\"])\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    filename = '{}_{}_{}_{:.3f}.pth'.format(cfg[\"model\"][\"name\"], cfg[\"model\"][\"backbone\"],epoch,mIoU)\n",
        "    filename = os.path.join(directory, filename)\n",
        "    if dataparallel:\n",
        "        model = model.module\n",
        "    #if is_best:\n",
        "    best_filename = '{}_{}_{}_{:.3f}_best_model.pth'.format(cfg[\"model\"][\"name\"], cfg[\"model\"][\"backbone\"],epoch,mIoU)\n",
        "    best_filename = os.path.join(directory, best_filename)\n",
        "    torch.save(\n",
        "        {\n",
        "            'model_state_dict': model.state_dict(), \n",
        "            'optim_state_dict': optimizer.state_dict(),\n",
        "            'current_epoch': epoch,\n",
        "            'valid_loss': valid_loss,\n",
        "            'valid_mIoU': valid_mIoU,\n",
        "            'train_loss': trainer.losses[-1],\n",
        "            'train_mIoU': trainer.IoU_list[-1]\n",
        "        }, best_filename)\n",
        "    save_to_drive(best_filename)\n",
        "        \n",
        "\n",
        "#if __name__ == '__main__':\n",
        "def main(load = False, path = None):\n",
        "    # Set config file\n",
        "    config_path = \"./icnet.yaml\"\n",
        "    with open(config_path, \"r\") as yaml_file:\n",
        "        cfg = yaml.full_load(yaml_file.read())\n",
        "        #print(cfg)\n",
        "        #print(cfg[\"model\"][\"backbone\"])\n",
        "        #print(cfg[\"train\"][\"specific_gpu_num\"])\n",
        "    \n",
        "    # Use specific GPU\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(cfg[\"train\"][\"specific_gpu_num\"])\n",
        "    num_gpus = len(cfg[\"train\"][\"specific_gpu_num\"].split(','))\n",
        "    print(\"torch.cuda.is_available(): {}\".format(torch.cuda.is_available()))\n",
        "    print(\"torch.cuda.device_count(): {}\".format(torch.cuda.device_count()))\n",
        "    print(\"torch.cuda.current_device(): {}\".format(torch.cuda.current_device()))\n",
        "\n",
        "    # Set logger\n",
        "    #logger = SetupLogger(name = \"semantic_segmentation\", \n",
        "     #                     save_dir = cfg[\"train\"][\"ckpt_dir\"], \n",
        "      #                    distributed_rank = 0,\n",
        "       #                   filename='{}_{}_log.txt'.format(cfg[\"model\"][\"name\"], cfg[\"model\"][\"backbone\"]))\n",
        "    #logger.info(\"Using {} GPUs\".format(num_gpus))\n",
        "    #logger.info(\"torch.cuda.is_available(): {}\".format(torch.cuda.is_available()))\n",
        "    #logger.info(\"torch.cuda.device_count(): {}\".format(torch.cuda.device_count()))\n",
        "    #logger.info(\"torch.cuda.current_device(): {}\".format(torch.cuda.current_device()))\n",
        "    #logger.info(cfg)\n",
        "    \n",
        "    # Start train\n",
        "    trainer = Trainer(cfg, load, path) # trainer = Trainer(cfg,logger)\n",
        "    model = trainer.train()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zq3e9IMeo6Qs"
      },
      "outputs": [],
      "source": [
        "\"\"\"Custom losses.\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "__all__ = ['ICNetLoss']\n",
        "\n",
        "# TODO: optim function\n",
        "class ICNetLoss(nn.CrossEntropyLoss):\n",
        "    \"\"\"Cross Entropy Loss for ICNet\"\"\"\n",
        "    \n",
        "    def __init__(self, aux_weight=0.4, ignore_index=-1):\n",
        "        super(ICNetLoss, self).__init__(ignore_index=ignore_index)\n",
        "        self.aux_weight = aux_weight\n",
        "\n",
        "    def forward(self, *inputs):\n",
        "        preds, target = tuple(inputs)\n",
        "        inputs = tuple(list(preds) + [target])\n",
        "\n",
        "        pred, pred_sub4, pred_sub8, pred_sub16, target = tuple(inputs)\n",
        "        # [batch, H, W] -> [batch, 1, H, W]\n",
        "        target = target.unsqueeze(1).float()\n",
        "        target_sub4 = F.interpolate(target, pred_sub4.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()\n",
        "        target_sub8 = F.interpolate(target, pred_sub8.size()[2:], mode='bilinear', align_corners=True).squeeze(1).long()\n",
        "        target_sub16 = F.interpolate(target, pred_sub16.size()[2:], mode='bilinear', align_corners=True).squeeze(\n",
        "            1).long()\n",
        "        loss1 = super(ICNetLoss, self).forward(pred_sub4, target_sub4)\n",
        "        loss2 = super(ICNetLoss, self).forward(pred_sub8, target_sub8)\n",
        "        loss3 = super(ICNetLoss, self).forward(pred_sub16, target_sub16)\n",
        "        #return dict(loss=loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight)\n",
        "        return loss1 + loss2 * self.aux_weight + loss3 * self.aux_weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NTxALf5xpFXV"
      },
      "outputs": [],
      "source": [
        "\"\"\"Popular Learning Rate Schedulers\"\"\"\n",
        "from __future__ import division\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from bisect import bisect_right\n",
        "\n",
        "__all__ = ['IterationPolyLR']\n",
        "\n",
        "class IterationPolyLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, target_lr=0, max_iters=0, power=0.9, last_epoch=-1):\n",
        "        self.target_lr = target_lr\n",
        "        self.max_iters = max_iters\n",
        "        self.power = power\n",
        "        super(IterationPolyLR, self).__init__(optimizer, last_epoch)\n",
        "        \n",
        "    def get_lr(self):\n",
        "        N = self.max_iters \n",
        "        T = self.last_epoch\n",
        "        factor = pow(1 - T / N, self.power)\n",
        "        # https://blog.csdn.net/mieleizhi0522/article/details/83113824\n",
        "        return [self.target_lr + (base_lr - self.target_lr) * factor for base_lr in self.base_lrs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ell2TX3PpG3x"
      },
      "outputs": [],
      "source": [
        "\"\"\"Evaluation Metrics for Semantic Segmentation\"\"\"\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "__all__ = ['SegmentationMetric', 'batch_pix_accuracy', 'batch_intersection_union',\n",
        "           'pixelAccuracy', 'intersectionAndUnion', 'hist_info', 'compute_score']\n",
        "\n",
        "\n",
        "class SegmentationMetric(object):\n",
        "    \"\"\"Computes pixAcc and mIoU metric scores\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nclass):\n",
        "        super(SegmentationMetric, self).__init__()\n",
        "        self.nclass = nclass\n",
        "        self.reset()\n",
        "\n",
        "    def update(self, preds, labels):\n",
        "        \"\"\"Updates the internal evaluation result.\n",
        "        Parameters\n",
        "        ----------\n",
        "        labels : 'NumpyArray' or list of `NumpyArray`\n",
        "            The labels of the data.\n",
        "        preds : 'NumpyArray' or list of `NumpyArray`\n",
        "            Predicted values.\n",
        "        \"\"\"\n",
        "\n",
        "        def evaluate_worker(self, pred, label):\n",
        "            correct, labeled = batch_pix_accuracy(pred, label)\n",
        "            inter, union = batch_intersection_union(pred, label, self.nclass)\n",
        "\n",
        "            self.total_correct += correct\n",
        "            self.total_label += labeled\n",
        "            if self.total_inter.device != inter.device:\n",
        "                self.total_inter = self.total_inter.to(inter.device)\n",
        "                self.total_union = self.total_union.to(union.device)\n",
        "            self.total_inter += inter\n",
        "            self.total_union += union\n",
        "\n",
        "        if isinstance(preds, torch.Tensor):\n",
        "            evaluate_worker(self, preds, labels)\n",
        "        elif isinstance(preds, (list, tuple)):\n",
        "            for (pred, label) in zip(preds, labels):\n",
        "                evaluate_worker(self, pred, label)\n",
        "        \n",
        "    def get(self):\n",
        "        \"\"\"Gets the current evaluation result.\n",
        "        Returns\n",
        "        -------\n",
        "        metrics : tuple of float\n",
        "            pixAcc and mIoU\n",
        "        \"\"\"\n",
        "        pixAcc = 1.0 * self.total_correct / (2.220446049250313e-16 + self.total_label)  # remove np.spacing(1)\n",
        "        IoU = 1.0 * self.total_inter / (2.220446049250313e-16 + self.total_union)\n",
        "        mIoU = IoU.mean().item()\n",
        "        return pixAcc, mIoU\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Resets the internal evaluation result to initial state.\"\"\"\n",
        "        self.total_inter = torch.zeros(self.nclass)\n",
        "        self.total_union = torch.zeros(self.nclass)\n",
        "        self.total_correct = 0\n",
        "        self.total_label = 0\n",
        "\n",
        "\n",
        "# pytorch version\n",
        "def batch_pix_accuracy(output, target):\n",
        "    \"\"\"PixAcc\"\"\"\n",
        "    # inputs are numpy array, output 4D, target 3D\n",
        "    predict = torch.argmax(output.long(), 1) + 1\n",
        "    target = target.long() + 1\n",
        "\n",
        "    pixel_labeled = torch.sum(target > 0).item()\n",
        "\n",
        "    try:\n",
        "        pixel_correct = torch.sum((predict == target) * (target > 0)).item()\n",
        "    except:\n",
        "        print(\"predict size: {}, target size: {}, \".format(predict.size(), target.size()))\n",
        "    assert pixel_correct <= pixel_labeled, \"Correct area should be smaller than Labeled\"\n",
        "    return pixel_correct, pixel_labeled\n",
        "    \n",
        "\n",
        "def batch_intersection_union(output, target, nclass):\n",
        "    \"\"\"mIoU\"\"\"\n",
        "    # inputs are numpy array, output 4D, target 3D\n",
        "    mini = 1\n",
        "    maxi = nclass\n",
        "    nbins = nclass\n",
        "    predict = torch.argmax(output, 1) + 1  # [N,H,W] \n",
        "    target = target.float() + 1            # [N,H,W] \n",
        "\n",
        "    predict = predict.float() * (target > 0).float()\n",
        "    intersection = predict * (predict == target).float()\n",
        "    # areas of intersection and union\n",
        "    # element 0 in intersection occur the main difference from np.bincount. set boundary to -1 is necessary.\n",
        "    area_inter = torch.histc(intersection.cpu(), bins=nbins, min=mini, max=maxi)\n",
        "    area_pred = torch.histc(predict.cpu(), bins=nbins, min=mini, max=maxi)\n",
        "    area_lab = torch.histc(target.cpu(), bins=nbins, min=mini, max=maxi)\n",
        "    area_union = area_pred + area_lab - area_inter\n",
        "    assert torch.sum(area_inter > area_union).item() == 0, \"Intersection area should be smaller than Union area\"\n",
        "    return area_inter.float(), area_union.float()\n",
        "\n",
        "\n",
        "def pixelAccuracy(imPred, imLab):\n",
        "    \"\"\"\n",
        "    This function takes the prediction and label of a single image, returns pixel-wise accuracy\n",
        "    To compute over many images do:\n",
        "    for i = range(Nimages):\n",
        "         (pixel_accuracy[i], pixel_correct[i], pixel_labeled[i]) = \\\n",
        "            pixelAccuracy(imPred[i], imLab[i])\n",
        "    mean_pixel_accuracy = 1.0 * np.sum(pixel_correct) / (np.spacing(1) + np.sum(pixel_labeled))\n",
        "    \"\"\"\n",
        "    # Remove classes from unlabeled pixels in gt image.\n",
        "    # We should not penalize detections in unlabeled portions of the image.\n",
        "    pixel_labeled = np.sum(imLab >= 0)\n",
        "    pixel_correct = np.sum((imPred == imLab) * (imLab >= 0))\n",
        "    pixel_accuracy = 1.0 * pixel_correct / pixel_labeled\n",
        "    return (pixel_accuracy, pixel_correct, pixel_labeled)\n",
        "\n",
        "\n",
        "def intersectionAndUnion(imPred, imLab, numClass):\n",
        "    \"\"\"\n",
        "    This function takes the prediction and label of a single image,\n",
        "    returns intersection and union areas for each class\n",
        "    To compute over many images do:\n",
        "    for i in range(Nimages):\n",
        "        (area_intersection[:,i], area_union[:,i]) = intersectionAndUnion(imPred[i], imLab[i])\n",
        "    IoU = 1.0 * np.sum(area_intersection, axis=1) / np.sum(np.spacing(1)+area_union, axis=1)\n",
        "    \"\"\"\n",
        "    # Remove classes from unlabeled pixels in gt image.\n",
        "    # We should not penalize detections in unlabeled portions of the image.\n",
        "    imPred = imPred * (imLab >= 0)\n",
        "\n",
        "    # Compute area intersection:\n",
        "    intersection = imPred * (imPred == imLab)\n",
        "    (area_intersection, _) = np.histogram(intersection, bins=numClass, range=(1, numClass))\n",
        "\n",
        "    # Compute area union:\n",
        "    (area_pred, _) = np.histogram(imPred, bins=numClass, range=(1, numClass))\n",
        "    (area_lab, _) = np.histogram(imLab, bins=numClass, range=(1, numClass))\n",
        "    area_union = area_pred + area_lab - area_intersection\n",
        "    return (area_intersection, area_union)\n",
        "\n",
        "\n",
        "def hist_info(pred, label, num_cls):\n",
        "    assert pred.shape == label.shape\n",
        "    k = (label >= 0) & (label < num_cls)\n",
        "    labeled = np.sum(k)\n",
        "    correct = np.sum((pred[k] == label[k]))\n",
        "\n",
        "    return np.bincount(num_cls * label[k].astype(int) + pred[k], minlength=num_cls ** 2).reshape(num_cls,\n",
        "                                                                                                 num_cls), labeled, correct\n",
        "\n",
        "\n",
        "def compute_score(hist, correct, labeled):\n",
        "    iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n",
        "    mean_IU = np.nanmean(iu)\n",
        "    mean_IU_no_back = np.nanmean(iu[1:])\n",
        "    freq = hist.sum(1) / hist.sum()\n",
        "    freq_IU = (iu[freq > 0] * freq[freq > 0]).sum()\n",
        "    mean_pixel_acc = correct / labeled\n",
        "\n",
        "    return iu, mean_IU, mean_IU_no_back, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Se26g29myvZB"
      },
      "outputs": [],
      "source": [
        "\"\"\"Base segmentation dataset\"\"\"\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image, ImageOps, ImageFilter\n",
        "\n",
        "__all__ = ['SegmentationDataset']\n",
        "\n",
        "class SegmentationDataset(object):\n",
        "    \"\"\"Segmentation Base Dataset\"\"\"\n",
        "\n",
        "    def __init__(self, root, split, mode, transform, base_size=1024, crop_size=720):\n",
        "        \"\"\"\n",
        "        root: string\n",
        "        split: string\n",
        "            'train', 'val' or 'test'\n",
        "        mode:\n",
        "        transform: callable, optional\n",
        "             A function that transforms the image\n",
        "        base_size:\n",
        "            shorter size will be resized between [short_size*0.5, short_size*2.0]\n",
        "        crop_size:\n",
        "            \n",
        "        \"\"\"\n",
        "        super(SegmentationDataset, self).__init__()\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.split = split\n",
        "        self.mode = mode if mode is not None else split\n",
        "        self.base_size = base_size\n",
        "        self.crop_size = crop_size\n",
        "\n",
        "    def _val_sync_transform(self, img, mask):\n",
        "        \"\"\"\n",
        "        outsize = self.crop_size\n",
        "        short_size = outsize\n",
        "        w, h = img.size\n",
        "        if w > h:\n",
        "            oh = short_size\n",
        "            ow = int(1.0 * w * oh / h)\n",
        "        else:\n",
        "            ow = short_size\n",
        "            oh = int(1.0 * h * ow / w)\n",
        "        img = img.resize((ow, oh), Image.BILINEAR)\n",
        "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
        "        # center crop\n",
        "        w, h = img.size\n",
        "        x1 = int(round((w - outsize) / 2.))\n",
        "        y1 = int(round((h - outsize) / 2.))\n",
        "        img = img.crop((x1, y1, x1 + outsize, y1 + outsize))\n",
        "        mask = mask.crop((x1, y1, x1 + outsize, y1 + outsize))\n",
        "        \"\"\"\n",
        "        # final transform\n",
        "        img, mask = self._img_transform(img), self._mask_transform(mask)\n",
        "        return img, mask\n",
        "        \n",
        "    def _sync_transform(self, img, mask):\n",
        "        # random mirror\n",
        "        if random.random() < 0.5:\n",
        "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        crop_size = self.crop_size\n",
        "        # random scale (short edge)\n",
        "        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n",
        "        w, h = img.size\n",
        "        if h > w:\n",
        "            ow = short_size\n",
        "            oh = int(1.0 * h * ow / w)\n",
        "        else:\n",
        "            oh = short_size\n",
        "            ow = int(1.0 * w * oh / h)\n",
        "        img = img.resize((ow, oh), Image.BILINEAR)\n",
        "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
        "        # pad crop\n",
        "        if short_size < crop_size:\n",
        "            padh = crop_size - oh if oh < crop_size else 0\n",
        "            padw = crop_size - ow if ow < crop_size else 0\n",
        "            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n",
        "            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=0)\n",
        "        # random crop crop_size\n",
        "        w, h = img.size\n",
        "        x1 = random.randint(0, w - crop_size)\n",
        "        y1 = random.randint(0, h - crop_size)\n",
        "        img = img.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
        "        mask = mask.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
        "        # gaussian blur as in PSP\n",
        "        if random.random() < 0.5:\n",
        "            img = img.filter(ImageFilter.GaussianBlur(radius=random.random()))\n",
        "        # final transform\n",
        "        img, mask = self._img_transform(img), self._mask_transform(mask)\n",
        "        return img, mask\n",
        "        \n",
        "    def _img_transform(self, img):\n",
        "        return np.array(img)\n",
        "    \n",
        "    def _mask_transform(self, mask):\n",
        "        return np.array(mask).astype('int32')\n",
        "    \n",
        "    @property\n",
        "    def num_class(self):\n",
        "        \"\"\"Number of categories.\"\"\"\n",
        "        return self.NUM_CLASS\n",
        "\n",
        "    @property\n",
        "    def pred_offset(self):\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7a68y24kyQ08"
      },
      "outputs": [],
      "source": [
        "# We might not need this. Not sure.\n",
        "\"\"\"Prepare Cityscapes dataset\"\"\"\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "#from .segbase import SegmentationDataset\n",
        "\n",
        "class CityscapesDataset(SegmentationDataset):\n",
        "    NUM_CLASS = 19\n",
        "    IGNORE_INDEX=-1\n",
        "    NAME = \"cityscapes\"\n",
        "\n",
        "     # image transform\n",
        "    \"\"\"\n",
        "        transforms.ToTensor():\n",
        "            Convert a ``PIL Image`` or ``numpy.ndarray`` to tensor.\n",
        "            Converts a PIL Image or numpy.ndarray (H x W x C) in the range\n",
        "            [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n",
        "    \"\"\"\n",
        "    input_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([.485, .456, .406], [.229, .224, .225])])\n",
        "\n",
        "    def __init__(self, root = './datasets/Cityscapes', split='train', base_size=1024, crop_size=720, mode=None, transform=input_transform):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "            root : string\n",
        "                Path to Cityscapes folder. Default is './datasets/Cityscapes'\n",
        "            split: string\n",
        "                'train', 'val' or 'test'\n",
        "            transform : callable, optional\n",
        "                A function that transforms the image\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        super(CityscapesDataset, self).__init__(root, split, mode, transform,base_size, crop_size)\n",
        "        assert os.path.exists(self.root), \"Error: data root path is wrong!\"\n",
        "        self.images, self.mask_paths = _get_city_pairs(self.root, self.split)\n",
        "        assert (len(self.images) == len(self.mask_paths))\n",
        "        if len(self.images) == 0:\n",
        "            raise RuntimeError(\"Found 0 images in subfolders of:\" + root + \"\\n\")\n",
        "        # _gtFine_labelIds.png\n",
        "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,\n",
        "                              23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
        "        # reference: https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py\n",
        "        # _gtFine_labelIds.png\n",
        "        self._key = np.array([-1, -1, -1, -1, -1, -1,\n",
        "                              -1, -1, 0, 1, -1, -1,\n",
        "                              2, 3, 4, -1, -1, -1,\n",
        "                              5, -1, 6, 7, 8, 9,\n",
        "                              10, 11, 12, 13, 14, 15,\n",
        "                              -1, -1, 16, 17, 18])\n",
        "        # [-1, ..., 33]\n",
        "        self._mapping = np.array(range(-1, len(self._key) - 1)).astype('int32')\n",
        "        \n",
        "    def _class_to_index(self, mask):\n",
        "        # assert the value\n",
        "        values = np.unique(mask)\n",
        "        for value in values:\n",
        "            assert (value in self._mapping)\n",
        "        # mask_mapping\n",
        "        index = np.digitize(mask.ravel(), self._mapping, right=True)\n",
        "        # key\n",
        "        return self._key[index].reshape(mask.shape)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.images[index]).convert('RGB')\n",
        "        if self.mode == 'test':\n",
        "            if self.transform is not None:\n",
        "                img = self.transform(img)\n",
        "            return img, os.path.basename(self.images[index])\n",
        "        mask = Image.open(self.mask_paths[index])\n",
        "        # synchrosized transform\n",
        "        if self.mode == 'train':\n",
        "            img, mask = self._sync_transform(img, mask)\n",
        "        elif self.mode == 'val':\n",
        "            img, mask = self._val_sync_transform(img, mask)\n",
        "        else:\n",
        "            assert self.mode == 'testval'\n",
        "            img, mask = self._img_transform(img), self._mask_transform(mask)\n",
        "        # general normalize and toTensor\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, mask, os.path.basename(self.images[index])\n",
        "        \n",
        "    #mask_transform\n",
        "    def _mask_transform(self, mask):\n",
        "        target = self._class_to_index(np.array(mask).astype('int32'))\n",
        "        return torch.LongTensor(np.array(target).astype('int32'))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    @property\n",
        "    def pred_offset(self):\n",
        "        return 0\n",
        "\n",
        "\"\"\"\n",
        "Citicapes:\n",
        "- leftImg8bit\n",
        "    - train\n",
        "        - aachen\n",
        "            - aachen_xxx_leftImg8bit.png\n",
        "            - ...\n",
        "        - ....      \n",
        "    - val\n",
        "        - frankfurt\n",
        "            - frankfurt_xxx_leftImg8bit.png\n",
        "            - ...\n",
        "        - ...\n",
        "    - test\n",
        "        - berloin\n",
        "            - berlin_xxx_leftImg8bit.png\n",
        "            - ...\n",
        "        - ...\n",
        "- gtFine \n",
        "    - train\n",
        "        - aachen\n",
        "            - aachen_xxx_gtFine_color.png\n",
        "            - aachen_xxx_gtFine_labelIds.png\n",
        "            - ...\n",
        "        - ....      \n",
        "    - val\n",
        "        - frankfurt\n",
        "            - frankfurt_xxx_gtFine_color.png\n",
        "            - frankfurt_xxx_gtFine_labelIds.png\n",
        "            - ...\n",
        "        - ...\n",
        "    - test\n",
        "        - berloin\n",
        "            - berloin_xxx_gtFine_color.png\n",
        "            - berloin_xxx_gtFine_labelIds.png\n",
        "            - ...\n",
        "        - ...\n",
        "- trainImages.txt\n",
        "- trainLabels.txt\n",
        "- valImages.txt\n",
        "- valLabels.txt\n",
        "- testImages.txt\n",
        "- testLabels.txt\n",
        "\"\"\"\n",
        "\n",
        "def _get_city_pairs(folder, split='train'):\n",
        "    def get_path_pairs(img_folder, mask_folder):\n",
        "        img_paths = []\n",
        "        mask_paths = []\n",
        "        for root, _, files in os.walk(img_folder):\n",
        "            for filename in files:\n",
        "                if filename.endswith('.png'):\n",
        "                    \"\"\"\n",
        "                    Example:\n",
        "                        root = \"./Cityscapes/leftImg8bit/train/aachen\"\n",
        "                        filename = \"aachen_xxx_leftImg8bit.png\"\n",
        "                        imgpath = \"./Cityscapes/leftImg8bit/train/aachen/aachen_xxx_leftImg8bit.png\"\n",
        "                        foldername = \"aachen\"\n",
        "                        maskname = \"aachen_xxx_gtFine_labelIds.png\"\n",
        "                        maskpath = \"./Cityscapes/gtFine/train/aachen/aachen_xxx_gtFine_labelIds\"\n",
        "                    \"\"\"\n",
        "                    imgpath = os.path.join(root, filename)\n",
        "                    foldername = os.path.basename(os.path.dirname(imgpath))\n",
        "                    maskname = filename.replace('leftImg8bit', 'gtFine_labelIds')\n",
        "                    maskpath = os.path.join(mask_folder, foldername, maskname)\n",
        "                    if os.path.isfile(imgpath) and os.path.isfile(maskpath):\n",
        "                        img_paths.append(imgpath)\n",
        "                        mask_paths.append(maskpath)\n",
        "                    else:\n",
        "                        print('cannot find the mask or image:', imgpath, maskpath)\n",
        "        print('Found {} images in the folder {}'.format(len(img_paths), img_folder))\n",
        "        return img_paths, mask_paths\n",
        "\n",
        "    if split in ('train', 'val'):\n",
        "        # \"./Cityscapes/leftImg8bit/train\" or \"./Cityscapes/leftImg8bit/val\"\n",
        "        img_folder = os.path.join(folder, 'leftImg8bit/' + split)\n",
        "        # \"./Cityscapes/gtFine/train\" or \"./Cityscapes/gtFine/val\"\n",
        "        mask_folder = os.path.join(folder, 'gtFine/' + split)\n",
        "        # img_paths,mask_paths = path pairs\n",
        "        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n",
        "        return img_paths, mask_paths\n",
        "    else:\n",
        "        assert split == 'trainval'\n",
        "        print('trainval set')\n",
        "        train_img_folder = os.path.join(folder, 'leftImg8bit/train')\n",
        "        train_mask_folder = os.path.join(folder, 'gtFine/train')\n",
        "        val_img_folder = os.path.join(folder, 'leftImg8bit/val')\n",
        "        val_mask_folder = os.path.join(folder, 'gtFine/val')\n",
        "        train_img_paths, train_mask_paths = get_path_pairs(train_img_folder, train_mask_folder)\n",
        "        val_img_paths, val_mask_paths = get_path_pairs(val_img_folder, val_mask_folder)\n",
        "        img_paths = train_img_paths + val_img_paths\n",
        "        mask_paths = train_mask_paths + val_mask_paths\n",
        "    return img_paths, mask_paths\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDiiEpUqT49y"
      },
      "source": [
        "# RUN THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "61JpK993IXHI",
        "outputId": "ded164c1-6057-4f0e-c77a-4d76ffbedc3b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-139cdb76388e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/multiclass-seg/icnet-scale1-30k/icnet_resnet50_71_0.642_best_model.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-97b4896afba7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(load, path)\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;31m# Set config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mconfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./icnet.yaml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m#print(cfg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './icnet.yaml'"
          ]
        }
      ],
      "source": [
        "load = False\n",
        "load_path = \"/content/drive/MyDrive/multiclass-seg/icnet-scale1-30k/icnet_resnet50_71_0.642_best_model.pth\"\n",
        "\n",
        "model = main(load, load_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubkvaF8GUkTk"
      },
      "source": [
        "# PLOT THE LOSSES AND SCORES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcgqTEx6jDa5"
      },
      "outputs": [],
      "source": [
        "plot_losses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljfFaBvrUitC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_losses():\n",
        "\n",
        "    path = \"/content/drive/MyDrive/multiclass-seg/icnet-scale1-base-20Epoch\" # CHANGE THIS PER MODEL\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_mIoUs = []\n",
        "    val_mIoUs = []\n",
        "\n",
        "    for item in os.listdir(path):\n",
        "      checkpoint = torch.load(path + \"/\" + item)\n",
        "      train_loss = checkpoint['train_loss']\n",
        "      train_mIoU = checkpoint['train_mIoU']\n",
        "      valid_loss = checkpoint['valid_loss']\n",
        "      valid_mIoU = checkpoint['valid_mIoU']\n",
        "      train_losses.append(train_loss)\n",
        "      val_losses.append(valid_loss)\n",
        "      train_mIoUs.append(train_mIoU)\n",
        "      val_mIoUs.append(valid_mIoU)\n",
        "\n",
        "    epoch_list = checkpoint['current_epoch']\n",
        "    epoch_list = list(range(epoch_list))\n",
        "\n",
        "    plt.plot(epoch_list, train_losses, color='blue', label = \"train\")\n",
        "    plt.plot(epoch_list, val_losses, color='red', label = \"test\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title(\"Loss over 20 epochs\")\n",
        "    plt.show()\n",
        "\n",
        "    print()\n",
        "\n",
        "    plt.plot(epoch_list, train_mIoUs, color='green', label = \"train\")\n",
        "    plt.plot(epoch_list, val_mIoUs, color='purple', label = \"test\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('mIoU')\n",
        "    plt.legend()\n",
        "    plt.title(\"mIoU over 20 epochs\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW_DbolSqGag"
      },
      "source": [
        "# VIDEO SEGMENTATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GUCLOTbvEO3G"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define the location of the image frames\n",
        "path = \"/content/drive/MyDrive/multiclass-seg/cityscapes/stuttgart_00\"\n",
        "\n",
        "# Get the list of image frames\n",
        "frames = [f for f in os.listdir(path) if f.endswith(\".png\")]\n",
        "\n",
        "# Sort the frames in order\n",
        "frames.sort(key=lambda x: int(x.split(\"_\")[-2]))\n",
        "\n",
        "# Define the parameters for the video writer\n",
        "frame_width = int(cv2.imread(os.path.join(path, frames[0])).shape[1])\n",
        "frame_height = int(cv2.imread(os.path.join(path, frames[0])).shape[0])\n",
        "fps = 17\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
        "\n",
        "# Create the video writer\n",
        "out = cv2.VideoWriter(\"video.mp4\", fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Write each frame to the video\n",
        "for frame in frames:\n",
        "    img = cv2.imread(os.path.join(path, frame))\n",
        "    out.write(img)\n",
        "\n",
        "# Release the video writer\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4DgUKzK4DYJ",
        "outputId": "08751c02-e1b5-4f6a-e9cf-e9b4d6dc41d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model_temp = torch.load(\"/content/drive/MyDrive/multiclass-seg/icnet-scale1-30k/icnet_resnet50_90_0.644_best_model.pth\", map_location='cuda:0')\n",
        "icnet_temp = ICNet(nclass = 19, backbone='resnet50').to(\"cuda:0\")\n",
        "icnet_temp.load_state_dict(model_temp['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWgMo7kD4esN"
      },
      "outputs": [],
      "source": [
        "def save_to_drive_file(source_path):\n",
        "  #print(source_path)\n",
        "  shutil.copy(source_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "hb2hsu2idMFx"
      },
      "outputs": [],
      "source": [
        "imagePath = \"/content/drive/MyDrive/multiclass-seg/cityscapes/stuttgart_00\" \n",
        "\n",
        "imagesTensor = []\n",
        "\n",
        "pathArray = []\n",
        "\n",
        "for image in os.listdir(imagePath):\n",
        "  path = imagePath + \"/\" + image\n",
        "  pathArray.append(path)\n",
        "  \n",
        "pathArray.sort(key=lambda x: int(x.split(\"_\")[-2]))\n",
        "\n",
        "realImages = []\n",
        "\n",
        "for image in pathArray:\n",
        "  real_image = cv2.imread(image)\n",
        "  realImages.append(real_image)\n",
        "  img = Image.open(image)\n",
        "  img = transforms.ToTensor()(img)\n",
        "  img = img.to(\"cuda\")\n",
        "  imagesTensor.append(img)\n",
        "\n",
        "  timesArray = [t*0.0588 for t in range(len(imagesTensor))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "68TMo3lx2O9U"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "imagePath = \"/content/drive/MyDrive/multiclass-seg/cityscapes/stuttgart_00\" \n",
        "\n",
        "save_path = \"/content/drive/MyDrive/multiclass-seg/cityscapes/save_path\" \n",
        "\n",
        "currentIndex = 0\n",
        "\n",
        "totalTime = 0\n",
        "\n",
        "outputTensor = []\n",
        "outputTimes = []\n",
        "\n",
        "for i, image in enumerate(imagesTensor):\n",
        "\n",
        "  if totalTime <= timesArray[currentIndex+1] and totalTime >= timesArray[currentIndex]:\n",
        "    totalTime = timesArray[currentIndex]\n",
        "    outputTimes.append(totalTime)\n",
        "    img = image.unsqueeze(0)\n",
        "    startTime = time.time()\n",
        "    pred = icnet_temp(img)[0]\n",
        "    endTime = time.time()\n",
        "    pred = pred.cpu().detach().numpy()\n",
        "    scores_matrix = np.argmax(pred[0], axis=0)\n",
        "    scores_matrix = scores_matrix.astype(np.uint8)\n",
        "    outputTensor.append(scores_matrix)\n",
        "    elapsedTime = endTime - startTime\n",
        "    totalTime += elapsedTime\n",
        "\n",
        "    if totalTime <= timesArray[currentIndex+1]:\n",
        "      totalTime = timesArray[currentIndex+1]\n",
        "  else:\n",
        "    currentIndex += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(outputTimes))\n",
        "print(len(timesArray))"
      ],
      "metadata": {
        "id": "-vJB56bCnfyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "CXTBmFAPUXng"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "color_map = np.array([[0, 0,  255], [ 255,  0, 0], [128, 128,128], [255, 0,127], [102,0,102], [255,255,51], [0, 255, 255], [102,0,51], [0,204,0], \n",
        "                      [180,165,180], [102,178,255], [0,102, 102], [153,153,153], [102,0,0], [250,170, 30], [220,220,  0], [107,142, 35], [152,251,152], \n",
        "                      [70,130,180], [102, 0, 51]])\n",
        "\n",
        "\n",
        "tensor = imagesTensor[0]\n",
        "frame_height, frame_width = tensor[0].shape  \n",
        "\n",
        "fps = 17\n",
        "\n",
        "\n",
        "mask_list = list(zip(outputTimes, outputTensor))\n",
        "\n",
        "# Sort the list of masks based on the frame times\n",
        "mask_list.sort(key=lambda x: x[0])\n",
        "\n",
        "# Define the output video file name\n",
        "out_video_file = \"output_video.mp4\"\n",
        "\n",
        "# Define the fourcc codec to be used\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "\n",
        "# Create a VideoWriter object\n",
        "out = cv2.VideoWriter(out_video_file, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# Write the segmentation masks to the video file\n",
        "for i, (frame_time, tensor) in enumerate(mask_list):\n",
        "    real_image = realImages[i]\n",
        "    # Convert the PyTorch tensor to a numpy array\n",
        "    # Create an empty image with the same shape as the mask and three color channels\n",
        "    frame = np.zeros((tensor.shape[0], tensor.shape[1], 3), dtype=np.uint8)\n",
        "    # Map the class indices in the mask to the corresponding colors in the color map\n",
        "    for j in range(color_map.shape[0]):\n",
        "        frame[tensor == j] = color_map[j]\n",
        "    # Convert the numpy array to a OpenCV image\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    #real_image = np.transpose(real_image, (1, 2, 0))\n",
        "    overlayed_image = cv2.addWeighted(frame, 0.7, real_image, 0.3, 0)\n",
        "    out.write(overlayed_image)\n",
        "    cv2.waitKey(int(frame_time * 1000 / fps))\n",
        "\n",
        "# Release the VideoWriter object\n",
        "out.release()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}